{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spoken language detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d758a0066910796d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b90391470eb7da4"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import IPython"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T18:22:16.275387Z",
     "start_time": "2024-05-22T18:22:14.115882Z"
    }
   },
   "id": "e771baad4b9578e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 20:22:14.438059: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-22 20:22:14.441024: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-22 20:22:14.479372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 20:22:15.213897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "PL_DIR = os.path.join('languages_audio', 'languages_audio_wav', 'pl', 'clips')\n",
    "PL_DIR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4343ebee9c38ae63",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# create tensorflow dataset\n",
    "pl_dataset = tf.data.Dataset.list_files(PL_DIR + '\\*.wav')\n",
    "pl_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba1dbf66c6eda4bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pl_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a641093a2c580c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pl_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e49cfbfba3bafb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(os.path.join('languages_audio', 'languages_audio_mp3', 'pl', 'validated.tsv'), sep='\\t')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b25c7c351f5d432",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['gender'] == 'female_feminine']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a419164919440f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['gender'] == 'male_masculine']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0ecc7926645f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupby('sentence_id').count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805a0e99ad4a6f32",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['sentence_id'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3765e7a31c9a897d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['sentence_id'].nunique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c679c99c892a10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['client_id'].value_counts().sort_values(ascending=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fedb03302fc130ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d7ac801357330b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[['sentence_id', 'client_id']].groupby(by='sentence_id').count().sort_values(by='client_id', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66628d65d38615e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[['client_id', 'sentence_id']].sort_values(by='client_id', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "788ead3deb0c642a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[['client_id', 'sentence_id']].sort_values(by='client_id', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54720b0b0d5e35f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['client_id'] == 'ffffc00a5ed69f08a486837064ec2caeff21fe06264c3dd733f633fb6c2ae9aeb561a5a6f43e000554d4b3cc171644ba4ce971177af1cb54f9bd8cc153e71a5c']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28a7c157bc223a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # sprawdzenie czy client_id oraz sequence_id się nie duplikują (czy jeden klient ma rózne sekwencej)\n",
    "# \n",
    "# for client_id in df['client_id'].unique():\n",
    "#     sentence_count = df['sentence_id'][df['client_id']  == client_id].count()\n",
    "#     client_id_count = df['client_id'][df['client_id']  == client_id].value_counts().iloc[0]\n",
    "#     if sentence_count != client_id_count:\n",
    "#         print(True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ec3f2d2259223d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['sentence_id'][df['client_id']  == 'ffffc00a5ed69f08a486837064ec2caeff21fe06264c3dd733f633fb6c2ae9aeb561a5a6f43e000554d4b3cc171644ba4ce971177af1cb54f9bd8cc153e71a5c'].count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "589f00618f423817",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['client_id'][df['client_id']  == 'ffffc00a5ed69f08a486837064ec2caeff21fe06264c3dd733f633fb6c2ae9aeb561a5a6f43e000554d4b3cc171644ba4ce971177af1cb54f9bd8cc153e71a5c'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "833e078c9b16e7e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: WZiąć plik validated.tsv nastepnie podzielić go na mężczyzn i kobiety (dwa osobne dataset). następnie wrzucać do każdego ze zbiorów (najpiew testwoy, walidacyjny, treningowy) client_id od najmniejszej liczby wystąpień w obydwu  dopasować czas trwania każdej próbki (najpierw obcinać od środka, potem dopełniać zerami), zmienić na spektogram i dopiero wtedy dodać (koniec funkcji). druga funkcja będzie jako augumentowanie losowych danych (losowo wybiera funkcję która coś zrobi z danymi) jeżeli moje zbiory kobiet i mężczyzn nie będą spełniały wymaganej ilości próbek którą się poda. Na wyjściu ma być tensor z próbkami spektogramu. To zrobić finalnie dla każdego folderu (języka). Wtedy dopiero dodać label w formie one hot encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b41eb727ab7bcbf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d07b8080fb6705c7"
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(os.path.join('..', '..', 'languages', 'lt', 'validated.tsv'), sep='\\t', usecols=['client_id', 'path', 'sentence_id', 'gender', 'locale'])",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:19.290355Z",
     "start_time": "2024-05-22T19:38:19.231165Z"
    }
   },
   "id": "1ba578e1c67f2d8d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change mp3 to wav name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f69ec6d7189b945"
  },
  {
   "cell_type": "code",
   "source": [
    "# df = df.apply(lambda path: path.str.replace('.mp3', '.wav'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:20.476883Z",
     "start_time": "2024-05-22T19:38:20.474718Z"
    }
   },
   "id": "9422e72ec897347b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split to woman and man sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3481591ddd6b352"
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:21.920234Z",
     "start_time": "2024-05-22T19:38:21.912609Z"
    }
   },
   "id": "437897c4a0dd254f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               client_id  \\\n",
       "0      05264b41636056b4c082890776306d78a64153a17761b0...   \n",
       "1      14aa994e528fe04b837f68f9bc5c6268a3a3e251496eb6...   \n",
       "2      40c8016380d4f3b6b6925bbb61e503513e9b6180a2d000...   \n",
       "3      5e0759b675e2e5eac5dda7e93e2c63cda8c508cdb6ee8b...   \n",
       "4      6259e29a1089285f79483569c1accf7d6d83217709a50a...   \n",
       "...                                                  ...   \n",
       "16638  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16639  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16640  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16641  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16642  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "\n",
       "                               path  \\\n",
       "0      common_voice_lt_25188648.mp3   \n",
       "1      common_voice_lt_25125425.mp3   \n",
       "2      common_voice_lt_25209338.mp3   \n",
       "3      common_voice_lt_26717831.mp3   \n",
       "4      common_voice_lt_26716253.mp3   \n",
       "...                             ...   \n",
       "16638  common_voice_lt_25162613.mp3   \n",
       "16639  common_voice_lt_25162614.mp3   \n",
       "16640  common_voice_lt_25162615.mp3   \n",
       "16641  common_voice_lt_25162616.mp3   \n",
       "16642  common_voice_lt_25162617.mp3   \n",
       "\n",
       "                                             sentence_id          gender  \\\n",
       "0      116a9dc506e249a0640eba08aef91cbe456d1c376566e0...             NaN   \n",
       "1      0aa7267734feda11b88b58076c6650f721ae067c99ac2f...             NaN   \n",
       "2      14ae0dafd4ce037b354ad30b910cd3a11bf7c1d7782121...             NaN   \n",
       "3      1aad0fa17da428819d8495eabf5ac5f522674bb82364c5...             NaN   \n",
       "4      1aef8e56b5672585da41597ce24d2b155287c5dc89cbf1...             NaN   \n",
       "...                                                  ...             ...   \n",
       "16638  10ef89654fb72a7b7b6d45deb809fb71216208a3f0f1ed...  male_masculine   \n",
       "16639  10db81bb49e598d5e22d2c8fafea545691d93d08fa25cf...  male_masculine   \n",
       "16640  1168759dfcaf33a91d6a0afbe2379542c1c2e5f04d4d42...  male_masculine   \n",
       "16641  1141867eb08255f5a2098121972fa6b3c8b3bc9b1807eb...  male_masculine   \n",
       "16642  11bca3bf711e438d046d5aaf69282b7583597d0e81c07a...  male_masculine   \n",
       "\n",
       "      locale  \n",
       "0         lt  \n",
       "1         lt  \n",
       "2         lt  \n",
       "3         lt  \n",
       "4         lt  \n",
       "...      ...  \n",
       "16638     lt  \n",
       "16639     lt  \n",
       "16640     lt  \n",
       "16641     lt  \n",
       "16642     lt  \n",
       "\n",
       "[16643 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05264b41636056b4c082890776306d78a64153a17761b0...</td>\n",
       "      <td>common_voice_lt_25188648.mp3</td>\n",
       "      <td>116a9dc506e249a0640eba08aef91cbe456d1c376566e0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14aa994e528fe04b837f68f9bc5c6268a3a3e251496eb6...</td>\n",
       "      <td>common_voice_lt_25125425.mp3</td>\n",
       "      <td>0aa7267734feda11b88b58076c6650f721ae067c99ac2f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40c8016380d4f3b6b6925bbb61e503513e9b6180a2d000...</td>\n",
       "      <td>common_voice_lt_25209338.mp3</td>\n",
       "      <td>14ae0dafd4ce037b354ad30b910cd3a11bf7c1d7782121...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e0759b675e2e5eac5dda7e93e2c63cda8c508cdb6ee8b...</td>\n",
       "      <td>common_voice_lt_26717831.mp3</td>\n",
       "      <td>1aad0fa17da428819d8495eabf5ac5f522674bb82364c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6259e29a1089285f79483569c1accf7d6d83217709a50a...</td>\n",
       "      <td>common_voice_lt_26716253.mp3</td>\n",
       "      <td>1aef8e56b5672585da41597ce24d2b155287c5dc89cbf1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162613.mp3</td>\n",
       "      <td>10ef89654fb72a7b7b6d45deb809fb71216208a3f0f1ed...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162614.mp3</td>\n",
       "      <td>10db81bb49e598d5e22d2c8fafea545691d93d08fa25cf...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162615.mp3</td>\n",
       "      <td>1168759dfcaf33a91d6a0afbe2379542c1c2e5f04d4d42...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162616.mp3</td>\n",
       "      <td>1141867eb08255f5a2098121972fa6b3c8b3bc9b1807eb...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162617.mp3</td>\n",
       "      <td>11bca3bf711e438d046d5aaf69282b7583597d0e81c07a...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16643 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "woman_filter = df['gender'] == 'female_feminine'\n",
    "man_filter = df['gender'] == 'male_masculine'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:29.061013Z",
     "start_time": "2024-05-22T19:38:29.056770Z"
    }
   },
   "id": "5ea864f56b635651",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "df_women = df[woman_filter]\n",
    "df_women"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:29.860172Z",
     "start_time": "2024-05-22T19:38:29.852933Z"
    }
   },
   "id": "283bff5acfcbbefb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               client_id  \\\n",
       "100    cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...   \n",
       "101    cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...   \n",
       "102    cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...   \n",
       "103    cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...   \n",
       "698    a515a8c82315570e3f5545b2eea5aa0465956e46ad4e67...   \n",
       "...                                                  ...   \n",
       "15549  3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...   \n",
       "15550  3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...   \n",
       "15551  3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...   \n",
       "15552  3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...   \n",
       "15553  3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...   \n",
       "\n",
       "                               path  \\\n",
       "100    common_voice_lt_38661032.mp3   \n",
       "101    common_voice_lt_38726139.mp3   \n",
       "102    common_voice_lt_38751511.mp3   \n",
       "103    common_voice_lt_39296587.mp3   \n",
       "698    common_voice_lt_27500874.mp3   \n",
       "...                             ...   \n",
       "15549  common_voice_lt_37151639.mp3   \n",
       "15550  common_voice_lt_37151641.mp3   \n",
       "15551  common_voice_lt_37151642.mp3   \n",
       "15552  common_voice_lt_37151644.mp3   \n",
       "15553  common_voice_lt_37151645.mp3   \n",
       "\n",
       "                                             sentence_id           gender  \\\n",
       "100    21d1c32a690ed9a5305049627c07701226c4880a9e02db...  female_feminine   \n",
       "101    234625b9eb49d522fa073ba4b76583834967ecdc645903...  female_feminine   \n",
       "102    2345c066c110689708ab601fd22bbe591adcb1d639e59d...  female_feminine   \n",
       "103    235ac923087ec86cdfd061afe10bed9aa0639216349715...  female_feminine   \n",
       "698    1bebd4807eef08d36117ccc93ae5fc8113964673c76a77...  female_feminine   \n",
       "...                                                  ...              ...   \n",
       "15549  1ef9701235800a049dac59735c3500084bdd990b475470...  female_feminine   \n",
       "15550  22ff5f79f44d032e01f928bb4b80f704c6fff526da1e24...  female_feminine   \n",
       "15551  220206e6a7825c0cc6d0693f80271eaedc4ffa0bcdb5b5...  female_feminine   \n",
       "15552  22dd83f76fc004685044a40fae5ae9821ed8316f2c4497...  female_feminine   \n",
       "15553  230bbda47738baf0ecef790bdd1ca50e7da8be30b350c4...  female_feminine   \n",
       "\n",
       "      locale  \n",
       "100       lt  \n",
       "101       lt  \n",
       "102       lt  \n",
       "103       lt  \n",
       "698       lt  \n",
       "...      ...  \n",
       "15549     lt  \n",
       "15550     lt  \n",
       "15551     lt  \n",
       "15552     lt  \n",
       "15553     lt  \n",
       "\n",
       "[3567 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...</td>\n",
       "      <td>common_voice_lt_38661032.mp3</td>\n",
       "      <td>21d1c32a690ed9a5305049627c07701226c4880a9e02db...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...</td>\n",
       "      <td>common_voice_lt_38726139.mp3</td>\n",
       "      <td>234625b9eb49d522fa073ba4b76583834967ecdc645903...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...</td>\n",
       "      <td>common_voice_lt_38751511.mp3</td>\n",
       "      <td>2345c066c110689708ab601fd22bbe591adcb1d639e59d...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>cb07d781da89d6a623a37c56addb82b9dfc8fd7ced0339...</td>\n",
       "      <td>common_voice_lt_39296587.mp3</td>\n",
       "      <td>235ac923087ec86cdfd061afe10bed9aa0639216349715...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>a515a8c82315570e3f5545b2eea5aa0465956e46ad4e67...</td>\n",
       "      <td>common_voice_lt_27500874.mp3</td>\n",
       "      <td>1bebd4807eef08d36117ccc93ae5fc8113964673c76a77...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...</td>\n",
       "      <td>common_voice_lt_37151639.mp3</td>\n",
       "      <td>1ef9701235800a049dac59735c3500084bdd990b475470...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...</td>\n",
       "      <td>common_voice_lt_37151641.mp3</td>\n",
       "      <td>22ff5f79f44d032e01f928bb4b80f704c6fff526da1e24...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...</td>\n",
       "      <td>common_voice_lt_37151642.mp3</td>\n",
       "      <td>220206e6a7825c0cc6d0693f80271eaedc4ffa0bcdb5b5...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...</td>\n",
       "      <td>common_voice_lt_37151644.mp3</td>\n",
       "      <td>22dd83f76fc004685044a40fae5ae9821ed8316f2c4497...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>3a79f164a09807bdeee7d660c3b2d76d692ad1ff57246a...</td>\n",
       "      <td>common_voice_lt_37151645.mp3</td>\n",
       "      <td>230bbda47738baf0ecef790bdd1ca50e7da8be30b350c4...</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3567 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "df_men = df[man_filter]\n",
    "df_men"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:38:32.410531Z",
     "start_time": "2024-05-22T19:38:32.403201Z"
    }
   },
   "id": "d227f88872caa27c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               client_id  \\\n",
       "22     77942f561de7e977fe0b4e02d2e519d8163be890fe4131...   \n",
       "23     77942f561de7e977fe0b4e02d2e519d8163be890fe4131...   \n",
       "24     77942f561de7e977fe0b4e02d2e519d8163be890fe4131...   \n",
       "25     8919c0b12315e6820dbe781233b7874dfbb97f2e6b7476...   \n",
       "26     8919c0b12315e6820dbe781233b7874dfbb97f2e6b7476...   \n",
       "...                                                  ...   \n",
       "16638  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16639  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16640  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16641  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "16642  e34db1479bd223145489573ff58e246c8b30e9de09bdc5...   \n",
       "\n",
       "                               path  \\\n",
       "22     common_voice_lt_37469401.mp3   \n",
       "23     common_voice_lt_37469402.mp3   \n",
       "24     common_voice_lt_37469406.mp3   \n",
       "25     common_voice_lt_38229315.mp3   \n",
       "26     common_voice_lt_38229318.mp3   \n",
       "...                             ...   \n",
       "16638  common_voice_lt_25162613.mp3   \n",
       "16639  common_voice_lt_25162614.mp3   \n",
       "16640  common_voice_lt_25162615.mp3   \n",
       "16641  common_voice_lt_25162616.mp3   \n",
       "16642  common_voice_lt_25162617.mp3   \n",
       "\n",
       "                                             sentence_id          gender  \\\n",
       "22     207751808ca5b0f52e111e57fc28271622b218e4702ae9...  male_masculine   \n",
       "23     22cb2c75e3721648d4a88ce3a4140410984acb061d9501...  male_masculine   \n",
       "24     226ef0cd92c7c885009f94f8430c26f3043ee1bc9099cc...  male_masculine   \n",
       "25     230dfcc19cf36a96901d00ed557f83302b1be803aadf59...  male_masculine   \n",
       "26     23375a307e5ecbcbd7361d6e9f961323101a07bc29a85c...  male_masculine   \n",
       "...                                                  ...             ...   \n",
       "16638  10ef89654fb72a7b7b6d45deb809fb71216208a3f0f1ed...  male_masculine   \n",
       "16639  10db81bb49e598d5e22d2c8fafea545691d93d08fa25cf...  male_masculine   \n",
       "16640  1168759dfcaf33a91d6a0afbe2379542c1c2e5f04d4d42...  male_masculine   \n",
       "16641  1141867eb08255f5a2098121972fa6b3c8b3bc9b1807eb...  male_masculine   \n",
       "16642  11bca3bf711e438d046d5aaf69282b7583597d0e81c07a...  male_masculine   \n",
       "\n",
       "      locale  \n",
       "22        lt  \n",
       "23        lt  \n",
       "24        lt  \n",
       "25        lt  \n",
       "26        lt  \n",
       "...      ...  \n",
       "16638     lt  \n",
       "16639     lt  \n",
       "16640     lt  \n",
       "16641     lt  \n",
       "16642     lt  \n",
       "\n",
       "[9681 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77942f561de7e977fe0b4e02d2e519d8163be890fe4131...</td>\n",
       "      <td>common_voice_lt_37469401.mp3</td>\n",
       "      <td>207751808ca5b0f52e111e57fc28271622b218e4702ae9...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>77942f561de7e977fe0b4e02d2e519d8163be890fe4131...</td>\n",
       "      <td>common_voice_lt_37469402.mp3</td>\n",
       "      <td>22cb2c75e3721648d4a88ce3a4140410984acb061d9501...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77942f561de7e977fe0b4e02d2e519d8163be890fe4131...</td>\n",
       "      <td>common_voice_lt_37469406.mp3</td>\n",
       "      <td>226ef0cd92c7c885009f94f8430c26f3043ee1bc9099cc...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8919c0b12315e6820dbe781233b7874dfbb97f2e6b7476...</td>\n",
       "      <td>common_voice_lt_38229315.mp3</td>\n",
       "      <td>230dfcc19cf36a96901d00ed557f83302b1be803aadf59...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8919c0b12315e6820dbe781233b7874dfbb97f2e6b7476...</td>\n",
       "      <td>common_voice_lt_38229318.mp3</td>\n",
       "      <td>23375a307e5ecbcbd7361d6e9f961323101a07bc29a85c...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162613.mp3</td>\n",
       "      <td>10ef89654fb72a7b7b6d45deb809fb71216208a3f0f1ed...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162614.mp3</td>\n",
       "      <td>10db81bb49e598d5e22d2c8fafea545691d93d08fa25cf...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162615.mp3</td>\n",
       "      <td>1168759dfcaf33a91d6a0afbe2379542c1c2e5f04d4d42...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162616.mp3</td>\n",
       "      <td>1141867eb08255f5a2098121972fa6b3c8b3bc9b1807eb...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>e34db1479bd223145489573ff58e246c8b30e9de09bdc5...</td>\n",
       "      <td>common_voice_lt_25162617.mp3</td>\n",
       "      <td>11bca3bf711e438d046d5aaf69282b7583597d0e81c07a...</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9681 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set number of probes in my set and split it to train, validate and test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf90c16bcfef86e1"
  },
  {
   "cell_type": "code",
   "source": [
    "SET_SIZE = 3_000\n",
    "MAX_NUMBER_OF_CLIENT_ID = 3000\n",
    "MIN_CLIP_DURATION = 4000 #  clip time in [ms]\n",
    "\n",
    "TRAIN_SIZE = int(SET_SIZE * 0.6)\n",
    "VAL_SIZE = int((SET_SIZE - TRAIN_SIZE) // 2)\n",
    "TEST_SIZE = SET_SIZE - TRAIN_SIZE - VAL_SIZE\n",
    "\n",
    "print(f'Train size: {TRAIN_SIZE}')\n",
    "print(f'Validation size: {VAL_SIZE}')\n",
    "print(f'Test size: {TEST_SIZE}')\n",
    "print(f'Sum of sizes: {TRAIN_SIZE + VAL_SIZE + TEST_SIZE} is equal to SET_SIZE {TRAIN_SIZE + VAL_SIZE + TEST_SIZE == SET_SIZE}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73b6742e55cd46ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Select probes with min that time \n",
    "df_clips_duration = pd.read_csv(os.path.join('..','..', 'languages', 'en', 'clip_durations.tsv'), sep='\\t')\n",
    "df_clips_duration['clip'] = df_clips_duration['clip']\n",
    "df_men = df_men.merge(df_clips_duration, left_on='path', right_on='clip')\n",
    "df_men"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc35c9fa1f2a04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# \n",
    "# for i in range(len(df_clips_duration)):\n",
    "#     print(i)\n",
    "#     row_in_df = df_men[df_men['path']== df_clips_duration.iloc[i]['clip']] \n",
    "#     if (df_clips_duration.iloc[i]['duration[ms]'] >= MIN_CLIP_DURATION) and (len(row_in_df) >= 0):\n",
    "#         rows_over_min_dur_time = pd.concat([rows_over_min_dur_time, row_in_df])\n",
    "#     else:\n",
    "#         rows_under_min_dur_time = pd.concat([rows_under_min_dur_time, row_in_df])\n",
    "# \n",
    "# \n",
    "\n",
    "rows_over_min_dur_time = df_men[df_men['duration[ms]'] >= MIN_CLIP_DURATION]\n",
    "rows_under_min_dur_time = df_men[df_men['duration[ms]'] < MIN_CLIP_DURATION]\n",
    "rows_over_min_dur_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a901ecef326bfb2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "rows_under_min_dur_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fe7b8e7b0273d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if len(rows_over_min_dur_time) >= SET_SIZE:\n",
    "    df_men = rows_over_min_dur_time\n",
    "else:\n",
    "    df_men = pd.concat([rows_over_min_dur_time, rows_under_min_dur_time], ignore_index=True)[:SET_SIZE]\n",
    "\n",
    "df_men"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a4ae1c66f24659e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_val = pd.DataFrame()\n",
    "df_test = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f0a7c187961064",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_counted_id = df_men['client_id'].value_counts(ascending=True)\n",
    "df_counted_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6219cdc9196dd3c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "clients_form_origin_df = df_men[df_men['client_id'] == df_counted_id.index[200]]\n",
    "clients_form_origin_df1 = df_men[df_men['client_id'] == df_counted_id.index[201]]\n",
    "clients_form_origin_df1['client_id'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d38e4396a58bfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(df_counted_id)):\n",
    "    print(df_counted_id.index[i], df_counted_id.iloc[i])\n",
    "    rows_form_origin_df = df_men[df_men['client_id'] == df_counted_id.index[i]][:MAX_NUMBER_OF_CLIENT_ID]\n",
    "    if len(rows_form_origin_df) <= (TEST_SIZE - len(df_test)) and rows_form_origin_df['client_id'].iloc[0]:\n",
    "        df_test = pd.concat([df_test, rows_form_origin_df], ignore_index=True)\n",
    "        continue\n",
    "    if len(df_test) < TEST_SIZE and rows_form_origin_df['client_id'].iloc[0]:\n",
    "        df_test = pd.concat([df_test, rows_form_origin_df[:(TEST_SIZE - len(df_test))]], ignore_index=True)\n",
    "        continue\n",
    "    if len(rows_form_origin_df) <= (VAL_SIZE - len(df_val)):\n",
    "        df_val = pd.concat([df_val, rows_form_origin_df], ignore_index=True)\n",
    "        continue\n",
    "    if len(df_val) < VAL_SIZE and rows_form_origin_df['client_id'].iloc[0]:\n",
    "        df_val = pd.concat([df_val, rows_form_origin_df[:(VAL_SIZE - len(df_val))]], ignore_index=True)\n",
    "        continue\n",
    "    if len(rows_form_origin_df) <= (TRAIN_SIZE - len(df_train)):\n",
    "        df_train = pd.concat([df_train, rows_form_origin_df], ignore_index=True)\n",
    "        continue\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "537212501acba841",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_test",
   "metadata": {
    "collapsed": false
   },
   "id": "cfc9212855ee88ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a10b1cb13de8b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d606986613d249",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_train['client_id'].value_counts(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dc1f3507e88ca70",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_val['client_id'].value_counts(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b76854c74dcbe6ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_test['client_id'].value_counts(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b852eeccef319e18",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29260740840bf9d"
  },
  {
   "cell_type": "code",
   "source": [
    "df_test_filenames = df_test['path']\n",
    "df_val_filenames = df_val['path']\n",
    "df_train_filenames = df_train['path']\n",
    "df_test_filenames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8924444a65be459",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad8dc801a01d1723",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_val_filenames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a365f15425586089",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "PL_DIR = os.path.join('..','..', 'languages', 'pl', 'clips')\n",
    "PL_DIR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0e645b674e76b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TEST\n",
    "df_train_filenames = df_train_filenames.apply(lambda fn: os.path.join(PL_DIR, fn))\n",
    "df_val_filenames = df_val_filenames.apply(lambda fn: os.path.join(PL_DIR, fn))\n",
    "df_test_filenames = df_test_filenames.apply(lambda fn: os.path.join(PL_DIR, fn))\n",
    "df_val_filenames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48837fb6e1737922",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# STARE\n",
    "# pl_dataset_train = tf.data.Dataset.from_tensor_slices(df_train_filenames.apply(lambda fn: os.path.join(PL_DIR, fn)))\n",
    "# pl_dataset_val = tf.data.Dataset.from_tensor_slices(df_val_filenames.apply(lambda fn: os.path.join(PL_DIR, fn)))\n",
    "# pl_dataset_test = tf.data.Dataset.from_tensor_slices(df_test_filenames.apply(lambda fn: os.path.join(PL_DIR, fn)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8152a30b0143953a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sample_rate = 48_000\n",
    "\n",
    "def add_zeros(wav, sample_rate):\n",
    "    time_probes = wav.shape[0]\n",
    "    missing_probes_one_side = int((MIN_CLIP_DURATION/1000* sample_rate - time_probes)//2)\n",
    "    padded_tensor = tf.pad(wav.numpy(), [[missing_probes_one_side, missing_probes_one_side]])\n",
    "    return tf.convert_to_tensor(padded_tensor, dtype=tf.float32)\n",
    "\n",
    "def cut_wav(wav, sample_rate):\n",
    "    time_probes = wav.shape[0]\n",
    "    # clip_dur_in_sec = time_probes / sample_rate\n",
    "    overlap = int((time_probes - (MIN_CLIP_DURATION/1000) * sample_rate)/2) \n",
    "    cut_clip = wav[overlap:(time_probes - overlap)]\n",
    "    return tf.convert_to_tensor(cut_clip, dtype=tf.float32)\n",
    "\n",
    "def load_wav_16k_mono_and_resample(filename, fin_sam_rate=16_000):\n",
    "    file_content = tf.io.read_file(filename)\n",
    "    wav = tfio.audio.decode_mp3(file_content)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    # sample_rate = tf.cast(sample_rate, dtype=tf.int32)\n",
    "    # wav = librosa.resample(wav.numpy(), orig_sr=sample_rate.numpy(), target_sr=fin_sam_rate)\n",
    "    # return wav\n",
    "    # return tf.convert_to_tensor(wav, dtype=tf.float32), sample_rate.numpy()\n",
    "    return tf.convert_to_tensor(wav, dtype=tf.float32)\n",
    "\n",
    "def load_and_align_probes(file_path):\n",
    "    wav = load_wav_16k_mono_and_resample(file_path)\n",
    "    expected_probes = int((MIN_CLIP_DURATION/1000) * sample_rate)\n",
    "    print(expected_probes)\n",
    "    current_probes = wav.shape[0]\n",
    "    print(current_probes)\n",
    "    if expected_probes > current_probes:\n",
    "        print(\"Add zeros\")\n",
    "        return add_zeros(wav, sample_rate)\n",
    "    elif expected_probes < current_probes:\n",
    "        print(\"Cut wav\")\n",
    "        return cut_wav(wav, sample_rate)\n",
    "    return tf.convert_to_tensor(wav, dtype=tf.float32)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8d53182f5b9bce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "59b0b2f915f9abdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "3500453bde18d26e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_align_dataset = tf.data.Dataset.from_tensor_slices(df_train_filenames.apply(lambda filename: load_and_align_probes(filename)).to_list())\n",
    "test_align_dataset = tf.data.Dataset.from_tensor_slices(df_test_filenames.apply(lambda filename: load_and_align_probes(filename)).to_list())\n",
    "val_align_dataset = tf.data.Dataset.from_tensor_slices(df_val_filenames.apply(lambda filename: load_and_align_probes(filename)).to_list())\n",
    "\n",
    "test_align_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b7f3fb18b5c989",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_align_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9b32aa07f7d3aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def align_probes(wav, sample_rate):\n",
    "    # wav = load_wav_16k_mono_and_resample(file_path)\n",
    "    expected_probes = (MIN_CLIP_DURATION/1000) * sample_rate\n",
    "    print(expected_probes)\n",
    "    current_probes = wav.shape[0]\n",
    "    print(current_probes)\n",
    "    if expected_probes > current_probes:\n",
    "        print(\"Add zeros\")\n",
    "        return add_zeros(wav, sample_rate)\n",
    "    if expected_probes < current_probes:\n",
    "        print(\"Cut wav\")\n",
    "        return cut_wav(wav, sample_rate)\n",
    "    return wav\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a38271117d59b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "MIN_CLIP_DURATION/1000* sample_rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94908a310416b0f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def increase_amplitude(wav, min_increase=2.0, max_increase=5.0):\n",
    "    increased_wav = wav * random.uniform(min_increase, max_increase)\n",
    "    return tf.convert_to_tensor(increased_wav, dtype=tf.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6111e6d7401e4a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_audio(wav):\n",
    "    max_amplitude = tf.reduce_max(tf.abs(wav))\n",
    "    normalized_wav = wav / max_amplitude  # Normalizacja do zakresu [-1, 1]\n",
    "    return normalized_wav"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9ffe4a7b8fc4f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def add_noise(wav, noise_level=0.1):\n",
    "    noise = tf.random.normal(tf.shape(wav), mean=0.0, stddev=noise_level)\n",
    "    return wav + noise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d8db06681bdb844",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def time_masking(wav, max_mask_length=10000):\n",
    "    # Sprawdzenie długości wav\n",
    "        # Sprawdzenie czy wav jest pusty lub ma nieprawidłowy kształt\n",
    "    if wav is None or tf.rank(wav) != 1:\n",
    "        return wav\n",
    "    \n",
    "    # Sprawdzenie długości wav\n",
    "    if tf.shape(wav)[0] <= max_mask_length:\n",
    "        return wav\n",
    "    \n",
    "    # Losowa długość maskowania\n",
    "    mask_length = tf.random.uniform([], maxval=max_mask_length, dtype=tf.int32)\n",
    "    \n",
    "    # Sprawdzenie, czy maska nie wyjdzie poza zakres\n",
    "    mask_start_max = tf.shape(wav)[0] - mask_length\n",
    "    mask_start = tf.random.uniform([], maxval=mask_start_max, dtype=tf.int32)\n",
    "    \n",
    "    # Stworzenie maski czasowej\n",
    "    mask = tf.concat([\n",
    "        tf.ones([mask_start]),\n",
    "        tf.zeros([mask_length]),\n",
    "        tf.ones([tf.shape(wav)[0] - mask_start - mask_length])\n",
    "    ], axis=0)\n",
    "    \n",
    "    # Zastosowanie maskowania do sygnału audio\n",
    "    masked_wav = wav * mask\n",
    "    \n",
    "    return masked_wav\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "144e847d5de32db0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def change_pitch(wav, sample_rate=48000, pitch_shift=2):\n",
    "    wav_np = wav.numpy()  # Convert tensor to numpy array\n",
    "    pitched_wav = librosa.effects.pitch_shift(wav_np, sr=sample_rate, n_steps=pitch_shift)\n",
    "    return tf.convert_to_tensor(pitched_wav, dtype=tf.float32)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81222bca1cef8ca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def speed_up_audio(wav, speed_factor=2):\n",
    "    wav_np = wav.numpy()\n",
    "    stretched_wav = librosa.effects.time_stretch(wav_np, rate=speed_factor)   \n",
    "    return stretched_wav"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67cf914574ec5490",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def slow_down_audio(wav, speed_factor=0.5):\n",
    "    wav_np = wav.numpy()\n",
    "    slowed_wav_np = librosa.effects.time_stretch(wav_np, rate=speed_factor)\n",
    "    return tf.convert_to_tensor(slowed_wav_np, dtype=tf.float32)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "375285678cc3b54b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "audio_processing_functions = [increase_amplitude, add_noise, time_masking]\n",
    "def process_random_samples(dataset, num_samples_to_process):\n",
    "    processed_samples = []\n",
    "    shuffled_dataset = dataset.shuffle(buffer_size=num_samples_to_process)\n",
    "    samples =  shuffled_dataset.take(num_samples_to_process)\n",
    "    for sample in samples:\n",
    "        processing_function = random.choice(audio_processing_functions)\n",
    "        processed_sample = processing_function(sample)\n",
    "    \n",
    "        processed_samples.append(processed_sample)\n",
    "\n",
    "    return processed_samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac294b85a18c5c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_samples_train = process_random_samples(train_align_dataset, TRAIN_SIZE - len(train_align_dataset))\n",
    "# processed_samples_val = process_random_samples(val_align_dataset, (VAL_SIZE - len(val_align_dataset)))\n",
    "# processed_samples_test = process_random_samples(test_align_dataset, (TEST_SIZE - len(test_align_dataset)))\n",
    "# processed_samples_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3798bc6bccff9b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "align_processed_train = [align_probes(tensor, sample_rate) for tensor in processed_samples_train]\n",
    "# align_processed_val = [align_probes(tensor, sample_rate) for tensor in processed_samples_val]\n",
    "# align_processed_test = [align_probes(tensor,sample_rate) for tensor in processed_samples_test]\n",
    "align_processed_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297274267826dace",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_samples_train_dataset = tf.data.Dataset.from_tensor_slices(align_processed_train)\n",
    "# processed_samples_val_dataset = tf.data.Dataset.from_tensor_slices(align_processed_val)\n",
    "# processed_samples_test_dataset = tf.data.Dataset.from_tensor_slices(align_processed_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a789f7eef861cb4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "78495d0f1138c0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "train_dataset_con = train_align_dataset.concatenate(processed_samples_train_dataset)\n",
    "# val_dataset_con = val_align_dataset.concatenate(processed_samples_val_dataset)\n",
    "# test_dataset_con = test_align_dataset.concatenate(processed_samples_test_dataset)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abb91118a9271c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "align_pl_dataset_train_with_processed_samples_norm = train_dataset_con.map(lambda audio: normalize_audio(audio))\n",
    "align_pl_dataset_val_with_processed_samples_norm = val_align_dataset.map(lambda audio: normalize_audio(audio))\n",
    "align_pl_dataset_test_with_processed_samples_norm = test_align_dataset.map(lambda audio: normalize_audio(audio))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3da5eb659b8031a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def create_spectrogram(wav):\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2de33c855dae8d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectrogram_pl_dataset_train = align_pl_dataset_train_with_processed_samples_norm.map(lambda audio: create_spectrogram(audio))\n",
    "spectrogram_pl_dataset_val = align_pl_dataset_val_with_processed_samples_norm.map(lambda audio: create_spectrogram(audio))\n",
    "spectrogram_pl_dataset_test = align_pl_dataset_test_with_processed_samples_norm.map(lambda audio: create_spectrogram(audio))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "994ed2ed4ea1fe63",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectrogram_pl_dataset_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91ece8457ac62112",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectrogram_pl_dataset_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e27210b5266e61",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectrogram_pl_dataset_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b97f35b8ad06f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spectrogram_pl_dataset_test.as_numpy_iterator().next().shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a896f7d090b76e59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "languages = ['pl', 'en', 'ls']\n",
    "language_to_index = {lang: idx for idx, lang in enumerate(languages)}\n",
    "print(language_to_index)\n",
    "\n",
    "def one_hot_encode_language(lang):\n",
    "    lang_index = language_to_index[lang]\n",
    "    one_hot = tf.one_hot(lang_index, len(languages))\n",
    "    return one_hot\n",
    "\n",
    "one_hot_encode_language('en')"
   ],
   "id": "31b6cfa1b4efe396",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labeled_train_dataset = tf.data.Dataset.zip((spectrogram_pl_dataset_train, tf.data.Dataset.from_tensor_slices([one_hot_encode_language('en')] * len(spectrogram_pl_dataset_train))))\n",
    "labeled_val_dataset = tf.data.Dataset.zip((spectrogram_pl_dataset_val, tf.data.Dataset.from_tensor_slices(tf.ones(len(spectrogram_pl_dataset_val)))))\n",
    "labeled_test_dataset = tf.data.Dataset.zip((spectrogram_pl_dataset_test, tf.data.Dataset.from_tensor_slices(tf.ones(len(spectrogram_pl_dataset_test)))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d46c6b640e6deaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labeled_train_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b930513b2d2ebfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labeled_val_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fd73f0fbab8bea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labeled_test_dataset.as_numpy_iterator().next()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddbc512a5fc9f74f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "568c51f7709b2f9b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
